{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################self attention################################\n",
      "k\n",
      " torch.Size([4, 8, 16])\n",
      "k.transpose(-2,-1)\n",
      " torch.Size([4, 16, 8])\n",
      "q\n",
      " torch.Size([4, 8, 16])\n",
      "w\n",
      " torch.Size([4, 8, 8])\n",
      "x\n",
      " torch.Size([4, 8, 32])\n",
      "out\n",
      " torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "w = torch.zeros(T, T)\n",
    "w = w.masked_fill(tril==0, float('-inf'))\n",
    "w = F.softmax(w, dim=-1)\n",
    "out = w@x\n",
    "# print(\"w\\n\", w)\n",
    "# print(\"x\\n\", x)\n",
    "# print(\"out\\n\", out)\n",
    "\n",
    "print(\"################################self attention################################\")\n",
    "import torch.nn as nn\n",
    "#implementing a single head to perform self-attention\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "print(\"k\\n\", k.shape)\n",
    "print(\"k.transpose(-2,-1)\\n\", k.transpose(-2,-1).shape)\n",
    "print(\"q\\n\", q.shape)\n",
    "w = q@k.transpose(-2,-1)#transposing the last two dimensions\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "w = w.masked_fill(tril==0, float('-inf'))\n",
    "w = F.softmax(w, dim=-1)\n",
    "v = value(x)\n",
    "out = w@v\n",
    "print(\"w\\n\", w.shape)\n",
    "print(\"x\\n\", x.shape)\n",
    "print(\"out\\n\", out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "65 characters in dataset\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "encoded: [46, 43, 50, 50, 53]\n",
      "decoded: hello\n",
      "torch.Size([1115393]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n",
      "Training data shape torch.Size([1003853])\n",
      "Validation data shape torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "#preparing the input data to the gpt model\n",
    "data_file = \"tiny_shakespeare.txt\"\n",
    "input_file = open(data_file, 'r')\n",
    "input_text = input_file.read()\n",
    "print(input_text[:100])\n",
    "#getting all the characters of the dataset\n",
    "chars = sorted(list(set(input_text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"{len(chars)} characters in dataset\")\n",
    "print(chars)\n",
    "\n",
    "#creating mapping from character to int and vice versa\n",
    "char2int = {c:i for i,c in enumerate(chars)}\n",
    "int2char = {i:c for i,c in enumerate(chars)}\n",
    "encoder = lambda s: [char2int[c] for c in s]\n",
    "decoder = lambda l: ''.join([int2char[i] for i in l])\n",
    "\n",
    "print(f\"encoded: {encoder('hello')}\")\n",
    "print(f\"decoded: {decoder(encoder('hello'))}\")\n",
    "#coding the text dataset and storing it as a pytorch tensor\n",
    "data_tensor = torch.tensor(encoder(input_text), dtype=torch.long)\n",
    "print(data_tensor.shape, data_tensor.dtype)\n",
    "print(data_tensor[:100])\n",
    "\n",
    "\n",
    "#splitting the dataset into training and validation sets\n",
    "train_size = 0.9\n",
    "train_data = data_tensor[:int(train_size*len(data_tensor))]\n",
    "val_data = data_tensor[int(train_size*len(data_tensor)):]\n",
    "print(\"Training data shape\", train_data.shape)\n",
    "print(\"Validation data shape\", val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            xb, yb = get_batch(split)\n",
    "            logits, loss = model(xb, yb)\n",
    "            losses[i] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_batch(split=\"train\"):\n",
    "    #getting a random batch from the test data or the validation data\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    #making sure we are staying within the length of the data\n",
    "    idxs = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    #getting the x and y values for the batch\n",
    "    xs = [data[idx:idx+block_size] for idx in idxs]\n",
    "    ys = [data[idx+1:idx+block_size+1] for idx in idxs]\n",
    "    #concatenating the tesnors and returning as a single tensor\n",
    "    return torch.stack(xs).to(device), torch.stack(ys).to(device)\n",
    "\n",
    "def train_model(epochs=3000):\n",
    "    losses = []\n",
    "    for steps in tqdm(range(epochs)):\n",
    "    \n",
    "        #getting the batch\n",
    "        xb, yb = get_batch(\"train\")\n",
    "        #evaluating the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # estimated_loss = estimate_loss()\n",
    "        losses.append(loss.item())\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self attention\"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        # computing the self attention scores\n",
    "        w = q @ k.transpose(-2,-1) * C**-0.5  # (B, T, T) and dividing by sqrt(C)\n",
    "        w = w.masked_fill(self.tril[:T,:T]==0, float('-inf')) # B, T, T\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        out = w @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd//n_head\n",
    "        self.sa_heads = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_heads(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GPTmodel, self).__init__()\n",
    "        self.token_embeddings_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embeddings_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        #idx and targets are both (B, T) shape tensors\n",
    "        token_embeddings = self.token_embeddings_table(idx) #(B, T, n_embd)\n",
    "        pos_embeddings = self.position_embeddings_table(torch.arange(T).to(device)) #(T, n_embd)\n",
    "        x = token_embeddings + pos_embeddings #(B, T, n_embd)\n",
    "        x = self.sa_heads(x) #(B, T, n_embd)\n",
    "        x = self.blocks(x) #(B, T, n_embd)\n",
    "        x = self.ln_f(x) #(B, T, n_embd)\n",
    "        logits = self.lm_head(x) #(B, T, vocab_size)\n",
    "        \n",
    "        if targets==None:\n",
    "            loss = None\n",
    "        else:        \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is a (B, T) shape array\n",
    "        for _ in range(max_new_tokens):\n",
    "            #we can not accomodate more than block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            #we are only interested in the last token of the sequence\n",
    "            #B, C  = logits.shape\n",
    "            logits = logits[:, -1, :]\n",
    "            #apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)# (B, 1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1) #(B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 1593/30000 [01:24<25:11, 18.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m train_model(\u001b[39m30000\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[56], line 38\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     36\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad(set_to_none\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 38\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     39\u001b[0m \u001b[39m# estimated_loss = estimate_loss()\u001b[39;00m\n\u001b[1;32m     40\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/projects/mi_projects/gpt_from_scratch/gpt_venv/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/mi_projects/gpt_from_scratch/gpt_venv/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/projects/mi_projects/gpt_from_scratch/gpt_venv/lib/python3.10/site-packages/torch/optim/adamw.py:171\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    161\u001b[0m         group,\n\u001b[1;32m    162\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m         state_steps,\n\u001b[1;32m    169\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m     adamw(\n\u001b[1;32m    172\u001b[0m         params_with_grad,\n\u001b[1;32m    173\u001b[0m         grads,\n\u001b[1;32m    174\u001b[0m         exp_avgs,\n\u001b[1;32m    175\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    176\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    177\u001b[0m         state_steps,\n\u001b[1;32m    178\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    179\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    180\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    181\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    182\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    183\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    184\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    185\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    186\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    187\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    188\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    189\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    190\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/projects/mi_projects/gpt_from_scratch/gpt_venv/lib/python3.10/site-packages/torch/optim/adamw.py:321\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 321\u001b[0m func(\n\u001b[1;32m    322\u001b[0m     params,\n\u001b[1;32m    323\u001b[0m     grads,\n\u001b[1;32m    324\u001b[0m     exp_avgs,\n\u001b[1;32m    325\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    326\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    327\u001b[0m     state_steps,\n\u001b[1;32m    328\u001b[0m     amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    329\u001b[0m     beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    330\u001b[0m     beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    331\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    332\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    333\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    334\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    335\u001b[0m     capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    336\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    337\u001b[0m     grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    338\u001b[0m     found_inf\u001b[39m=\u001b[39;49mfound_inf,\n\u001b[1;32m    339\u001b[0m )\n",
      "File \u001b[0;32m~/projects/mi_projects/gpt_from_scratch/gpt_venv/lib/python3.10/site-packages/torch/optim/adamw.py:477\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m differentiable, \u001b[39m\"\u001b[39m\u001b[39m_foreach ops don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt support autograd\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    475\u001b[0m \u001b[39massert\u001b[39;00m grad_scale \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m found_inf \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m grouped_tensors \u001b[39m=\u001b[39m _group_tensors_by_device_and_dtype([\n\u001b[1;32m    478\u001b[0m     params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps])\n\u001b[1;32m    479\u001b[0m \u001b[39mfor\u001b[39;00m (device_params, device_grads, device_exp_avgs, device_exp_avg_sqs,\n\u001b[1;32m    480\u001b[0m      device_max_exp_avg_sqs, device_state_steps) \u001b[39min\u001b[39;00m grouped_tensors\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    481\u001b[0m     \u001b[39mif\u001b[39;00m maximize:\n",
      "File \u001b[0;32m~/projects/mi_projects/gpt_from_scratch/gpt_venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/projects/mi_projects/gpt_from_scratch/gpt_venv/lib/python3.10/site-packages/torch/utils/_foreach_utils.py:33\u001b[0m, in \u001b[0;36m_group_tensors_by_device_and_dtype\u001b[0;34m(tensorlistlist, with_indices)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(tensorlistlist)):\n\u001b[1;32m     31\u001b[0m     \u001b[39m# a tensorlist may be empty/None\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[39mif\u001b[39;00m tensorlistlist[j]:\n\u001b[0;32m---> 33\u001b[0m         per_device_and_dtype_tensors[key][j]\u001b[39m.\u001b[39;49mappend(tensorlistlist[j][i])\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m with_indices:\n\u001b[1;32m     35\u001b[0m     \u001b[39m# tack on previous index\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     per_device_and_dtype_tensors[key][j \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mappend(i)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GPTmodel()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_model(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WYNCELINSA:\n",
      "Ocowild, is now the adye my be to tanth ther me?\n",
      "\n",
      "Tanss:\n",
      "Wart he usqurt tet? he that ane swick my thanst com orough ofform'd fitie ble mil;\n",
      "I lin, es ir ensen cintlatisel drove the the knong:\n",
      "Wilerabous lelind me letthule on hiry: ture aisstenw yeanlvdivs noamd atniivds ho:\n",
      "theut mitth kaloal----------\n",
      " iart woe my ournt hat hive he poorm we-forg\n",
      " tho dantnrupt for arcion tnmeftt, ey ale ont too durnkd mo of.\n",
      "\n",
      "HOMPure!\n",
      "Ifoordius ards buthes geestthuint muko ard ny Rrauts chave foardy, neetnnt by ary thou inth watht so a anths torey; ant weatht reand thou abcdies tnnly thever.\n",
      "\n",
      "\n",
      "Cuch freamltss\n",
      "atckin, mo\n",
      "IEnastheut peaktke shuedp eabvee oyournd?\n",
      "Cafd\n",
      "\n",
      "\n",
      "AMPestougheir Hoie st umdeakd ir eRpCaass:\n",
      "IbRont, ivecsss?\n",
      "KRINGl:\n",
      "I It teis omg tordanth.\n",
      "I wo a worlviun weat ko lwith self mo, doot wllo I mear yourcdseirgsuedkweels uhd poarvtheer.f Ctamey\n",
      "Heod him istius dthe nig the snt nakinscteer isubvot, a tthe sirst,\n",
      "Bet likhe ont an vuctheavbed\n",
      "BAGtUNT:\n",
      "MyOnYut irnft! Geyourg gioel\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long).to(device)\n",
    "print(decoder(model.generate(idx, 1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
